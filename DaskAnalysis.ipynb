{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:35149' processes=4 threads=12, memory=33.30 GB>"
      ],
      "text/html": "<table style=\"border: 2px solid white;\">\n<tr>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Client</h3>\n<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n  <li><b>Scheduler: </b>tcp://127.0.0.1:35149</li>\n  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n</ul>\n</td>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Cluster</h3>\n<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n  <li><b>Workers: </b>4</li>\n  <li><b>Cores: </b>12</li>\n  <li><b>Memory: </b>33.30 GB</li>\n</ul>\n</td>\n</tr>\n</table>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Création d'un client pour les outils de visualisation\n",
    "from dask.distributed import Client, progress\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "source": [
    "# Chargement des dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## YouGov - Wearing Mask in public"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Le dataset contient 192 enregistrements\nSample dataset final:\n    DateTime    country  percent_wearing_mask\n0 2020-02-21  Australia                   0.0\n1 2020-02-22  Australia                   0.0\n2 2020-02-23  Australia                   0.0\n3 2020-02-24  Australia                   0.0\n4 2020-02-25  Australia                   0.0\nTemps de chargement et tranformation petit dataset :  456.46 ms\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "\n",
    "##Chargement dataset\n",
    "df = dd.read_csv(\n",
    "    \"./data/wearing_face_mask_public.csv\",\n",
    "    sep=\";\"\n",
    ")\n",
    "\n",
    "##Transformation du dataset = 1 ligne par date/pays\n",
    "format = '%Y-%m-%d %H:%M:%S'\n",
    "df['DateTime'] = dd.to_datetime(df['DateTime'], format=format)\n",
    "df['DateTime'] = df['DateTime'].dt.normalize()\n",
    "\n",
    "##### 1er changement: sort_index et sort_values n'existe pas, d'ailleurs Dask ne supporte pas le tri sur de multiple colonnes\n",
    "##### Dans ce cas le sort_values n'était pas nécessaire et le groupby non plus\n",
    "# df = df.sort_values('DateTime').groupby(df['DateTime']).max()\n",
    "df = df.set_index('DateTime')\n",
    "##### 2e changement: pad n'existe pas sur un resample (Dask Resampler) seules les fonctions de downsampling sont implémentées\n",
    "# df = df.resample('1D').pad()\n",
    "wearing_mask_in_public_data = df.resample('1D').last()\n",
    "\n",
    "wearing_mask_in_public_data = wearing_mask_in_public_data.fillna(0)\n",
    "wearing_mask_in_public_data = wearing_mask_in_public_data.reset_index().melt(\n",
    "                                id_vars=['DateTime'], \n",
    "                                var_name='country', \n",
    "                                value_name='percent_wearing_mask')\n",
    "\n",
    "print(f\"Le dataset contient {len(df)} enregistrements\")\n",
    "\n",
    "##### 3e changement avec Dask : la doc indique que df.sample(5) n'est pas accepté, le paramère \"n\"\n",
    "##### ne doit pas être utlisé. Il faut utiliser \"frac\"\n",
    "##### De plus le \"print\" d'un Dataframe Dask n'affichera pas toutes les valeurs mais les types\n",
    "##### Enfin pour profiter du laziness on n'affichera pas avant la fin\n",
    "print(\"Sample dataset final:\")\n",
    "print(wearing_mask_in_public_data.head(5))\n",
    "\n",
    "stop = datetime.now()\n",
    "\n",
    "print(\"Temps de chargement et tranformation petit dataset : \", (stop-start).microseconds/1000, \"ms\")"
   ]
  },
  {
   "source": [
    "## Google - Covid 19 Open Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Partitions avant:  1\n",
      "Partitions après:  2\n",
      "Le dataset contient 21505 enregistrements\n",
      "Sample dataset final:\n",
      "   date  country_name  new_confirmed  new_deceased  new_recovered  new_tested  \\\n",
      "0     0             4           35.0           0.0            0.0         0.0   \n",
      "1     0           232         1313.0           2.0            0.0    149798.0   \n",
      "2     0             0          202.0          16.0            0.0         0.0   \n",
      "3     0             0           14.0           3.0            5.0         0.0   \n",
      "4     0             0            7.0           0.0            0.0         0.0   \n",
      "\n",
      "   total_confirmed  total_deceased  total_recovered  total_tested  ...  \\\n",
      "0           7162.0            78.0              0.0           0.0  ...   \n",
      "1         180150.0           598.0              0.0    17799178.0  ...   \n",
      "2          48053.0          1935.0              0.0           0.0  ...   \n",
      "3           3207.0           133.0           1836.0           0.0  ...   \n",
      "4            958.0            12.0            707.0           0.0  ...   \n",
      "\n",
      "   noaa_station  noaa_distance  average_temperature  minimum_temperature  \\\n",
      "0  8.117100e+09      39.866801             4.041667             0.604167   \n",
      "1  4.121610e+10      16.303110            24.455556            21.700000   \n",
      "2  4.094210e+10      85.393075             1.361111            -0.583333   \n",
      "3  4.091110e+10       8.359524             4.401235             1.456790   \n",
      "4  4.094510e+10      52.331322             3.453704            -0.027778   \n",
      "\n",
      "   maximum_temperature   rainfall  snowfall  dew_point  relative_humidity  \\\n",
      "0             6.687500   7.438571     30.48   1.486111          84.411044   \n",
      "1            27.461111   0.000000      0.00  15.072222          55.996506   \n",
      "2             3.861111  20.955000     10.16  -2.027778          78.490316   \n",
      "3             6.790123   0.825500     20.32  -0.135802          73.674285   \n",
      "4             6.537037  10.731500      0.00  -1.000000          73.894895   \n",
      "\n",
      "   percent_wearing_mask  \n",
      "0                   0.0  \n",
      "1                   0.0  \n",
      "2                   0.0  \n",
      "3                   0.0  \n",
      "4                   0.0  \n",
      "\n",
      "[5 rows x 96 columns]\n",
      "Temps de chargement et tranformation grand dataset :  618.293 ms\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#Chargement dataset\n",
    "\n",
    "##### 4e changement avec Dask : inférence de type\n",
    "# On va spécifier les types en erreur\n",
    "types = {\"locality_code\": str, \"locality_name\": str, \"subregion1_code\": str, \"subregion1_name\": str, \"subregion2_name\": str, \"subregion2_code\": str}\n",
    "covid19_opendata = dd.read_csv(\n",
    "    \"./data/latest.csv\",\n",
    "    keep_default_na=False,\n",
    "    na_values=[\"\"],\n",
    "    dtype=types,\n",
    "    sample=10000000)\n",
    "\n",
    "\n",
    "\n",
    "# Jointure entre open data covid 19 et yougo\n",
    "#covid19_opendata['date'] = pd.to_datetime(covid19_opendata['date'], format=format)\n",
    "covid19_opendata['date'] = covid19_opendata['date'].astype('M8[D]')\n",
    "\n",
    "covid19_merge1 = covid19_opendata.merge(wearing_mask_in_public_data, \n",
    "                                      left_on = ['country_name','date'],\n",
    "                                      right_on = ['country','DateTime'], how = 'left')\n",
    "\n",
    "\n",
    "remove_cols = ['key', 'country','aggregation_level','locality_code', 'wikidata', 'datacommons', 'country_code', 'subregion1_code', 'subregion1_name', 'subregion2_code', 'subregion2_name', 'locality_name', '3166-1-alpha-2', '3166-1-alpha-3', 'DateTime']\n",
    "\n",
    "covid19_merge1 = covid19_merge1.drop(remove_cols, axis=1)\n",
    "covid19_merge1 = covid19_merge1.fillna(0)\n",
    "\n",
    "#prepared_data =  covid19_merge1.copy()\n",
    "#### 5e changement : exécuté avec des \"workers\" qui acceptent 8Go max, il y aura une consommation excessive de la mémoire\n",
    "#### On avait 146 partitions, on diminue de moitié la taille des partitions actuelles\n",
    "print(\"Partitions avant: \", covid19_merge1.npartitions)\n",
    "covid19_merge1 = covid19_merge1.repartition(npartitions=covid19_merge1.npartitions * 2)\n",
    "print(\"Partitions après: \", covid19_merge1.npartitions)\n",
    "\n",
    "#print(\"covid19_merge1 partition :\",covid19_merge1.npartitions)\n",
    "prepared_data = client.persist(covid19_merge1)\n",
    "\n",
    "\n",
    "## Encode Pays\n",
    "from dask_ml.preprocessing import LabelEncoder\n",
    "encoded_countries = LabelEncoder().fit_transform(prepared_data.country_name)\n",
    "prepared_data['country_name'] = encoded_countries\n",
    "\n",
    "## Encode Date\n",
    "dates = prepared_data.date.apply(lambda x: x.strftime('%Y%m%d'))\n",
    "encoded_dates = LabelEncoder().fit_transform(dates)\n",
    "prepared_data['date'] = encoded_dates\n",
    "\n",
    "print(f\"Le dataset contient {len(prepared_data)} enregistrements\")\n",
    "\n",
    "print(\"Sample dataset final:\")\n",
    "print(prepared_data.head(5))\n",
    "\n",
    "stop = datetime.now()\n",
    "print(\"Temps de chargement et tranformation grand dataset : \", (stop-start).microseconds/1000, \"ms\")"
   ]
  },
  {
   "source": [
    "## Entrainement et inférence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 1836839.48287408\n",
      "Iteration 2, loss = 1816336.80127195\n",
      "Iteration 3, loss = 1695223.98896600\n",
      "Iteration 4, loss = 1579718.62825559\n",
      "Iteration 5, loss = 1502318.76508037\n",
      "Iteration 6, loss = 1391219.13944013\n",
      "Iteration 7, loss = 1344683.56211548\n",
      "Iteration 8, loss = 1239716.37406559\n",
      "Iteration 9, loss = 1110550.82691831\n",
      "Iteration 10, loss = 999077.70532076\n",
      "Temps préparation et inférence (ML) :  870.991 ms\n",
      "model score: -0.3247327998924583\n"
     ]
    }
   ],
   "source": [
    "#### 6e changement : Utilisation d'un backend spécifique pour dask\n",
    "import joblib\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# Split Train/Testmain\n",
    "from dask_ml.model_selection import train_test_split\n",
    "X = prepared_data.loc[:, prepared_data.columns != 'new_confirmed']    \n",
    "\n",
    "#### 7e changement : pour la création des labels Y, conversion de Series en Dask array\n",
    "y = prepared_data['new_confirmed']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "\n",
    "# Scale des valeurs\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "regr = MLPRegressor(max_iter=10, hidden_layer_sizes=(100, 50, 25, 10, 5), verbose=True)\n",
    "\n",
    "#### 8e changement : Parallélisme pour l'entrainement et la prédiction\n",
    "with joblib.parallel_backend('dask'):\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction et Score\n",
    "with joblib.parallel_backend('dask'):\n",
    "    score = regr.score(X_test, y_test)\n",
    "\n",
    "stop = datetime.now()\n",
    "\n",
    "print(\"Temps préparation et inférence (ML) : \", (stop-start).microseconds/1000, \"ms\")\n",
    "print(f\"model score: {score}\")"
   ]
  },
  {
   "source": [
    "Seul le training et le scale est parallélisé par Dask car le MLPRegressor n'a pas d'implémentation \"Dask\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Entrainement et inférence avec pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 1977091.78176523\n",
      "Iteration 2, loss = 1919877.98337147\n",
      "Iteration 3, loss = 1803444.93709255\n",
      "Iteration 4, loss = 1685347.15511803\n",
      "Iteration 5, loss = 1607300.66981109\n",
      "Iteration 6, loss = 1504542.59059035\n",
      "Iteration 7, loss = 1464055.62769924\n",
      "Iteration 8, loss = 1393318.96994639\n",
      "Iteration 9, loss = 1303947.25572069\n",
      "Iteration 10, loss = 1179337.96389524\n",
      "Temps préparation et inférence (ML) :  211.624 ms\n",
      "model score: -0.05028567998430855\n"
     ]
    }
   ],
   "source": [
    "#### 6e changement : Utilisation d'un backend spécifique pour dask\n",
    "import joblib\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# Split Train/Test\n",
    "from dask_ml.model_selection import train_test_split\n",
    "X = prepared_data.loc[:, prepared_data.columns != 'new_confirmed']\n",
    "\n",
    "#### 7e changement : pour la création des labels Y, conversion de Series en Dask array\n",
    "y = prepared_data['new_confirmed']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Scale des valeurs\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "scaler = StandardScaler()\n",
    "regr = MLPRegressor(max_iter=10, hidden_layer_sizes=(100, 50, 25, 10, 5), verbose=True)\n",
    "\n",
    "pipeline = Pipeline([('scaler', scaler), ('regressor', regr)])\n",
    "\n",
    "# Exécution du pipeline\n",
    "with joblib.parallel_backend('dask'):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction et Score\n",
    "with joblib.parallel_backend('dask'):\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "\n",
    "stop = datetime.now()\n",
    "\n",
    "print(\"Temps préparation et inférence (ML) : \", (stop-start).microseconds/1000, \"ms\")\n",
    "print(f\"model score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}