{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OldWayAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOu2BKs3sGKz"
      },
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "source": [
        "# Chargement des dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## YouGov - Wearing Mask in public\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "cvyiAclHs75I",
        "outputId": "c4543f8b-632e-4a27-ec58-7a895ce648ec"
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "\n",
        "##Chargement dataset\n",
        "df = pd.read_csv(\n",
        "    \"./data/wearing_face_mask_public.csv\",\n",
        "    sep=\";\"\n",
        ")\n",
        "\n",
        "\n",
        "## Transformation du dataset = 1 ligne par date/pays\n",
        "# On ne garde qu'une seule valeur par jour : le max pour chaque pays\n",
        "format = '%Y-%m-%d %H:%M:%S'\n",
        "df['DateTime'] = pd.to_datetime(df['DateTime'], format=format)\n",
        "df['DateTime'] = df['DateTime'].dt.normalize()\n",
        "\n",
        "df = df.sort_values('DateTime').groupby(df['DateTime']).max()\n",
        "df = df.set_index(pd.DatetimeIndex(df['DateTime'])).drop(['DateTime'], axis=1)\n",
        "wearing_mask_in_public_data = df.resample('1D').pad()\n",
        "wearing_mask_in_public_data = wearing_mask_in_public_data.fillna(0)\n",
        "wearing_mask_in_public_data = wearing_mask_in_public_data.reset_index().melt(\n",
        "                                id_vars=['DateTime'], \n",
        "                                var_name='country', \n",
        "                                value_name='percent_wearing_mask')\n",
        "\n",
        "\n",
        "print(f\"Le dataset contient {len(df)} enregistrements\")\n",
        "\n",
        "print(\"Sample dataset final:\")\n",
        "print(wearing_mask_in_public_data.sample(5))\n",
        "\n",
        "stop = datetime.now()\n",
        "\n",
        "print(\"Temps de chargement et tranformation petit dataset : \", (stop-start).microseconds/1000, \"ms\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le dataset contient 192 enregistrements\nSample dataset final:\n       DateTime    country  percent_wearing_mask\n2492 2020-10-30      India                   0.0\n2054 2020-05-25  Hong Kong                  87.0\n3551 2020-08-30   Malaysia                   0.0\n3632 2020-11-19   Malaysia                   0.0\n2474 2020-10-12      India                   0.0\nTemps de chargement et tranformation petit dataset :  22.862 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80VYFInzIP_D"
      },
      "source": [
        "## Google - Covid 19 Open Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU-rjehWIXgC",
        "outputId": "76af1806-4826-4a56-e168-e0f2a980b1b5"
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "#Chargement dataset\n",
        "covid19_opendata = pd.read_csv(\n",
        "    \"./data/latest.csv\",\n",
        "    keep_default_na=False,\n",
        "    na_values=[\"\"])\n",
        "\n",
        "\n",
        "# Jointure entre open data covid 19 et yougo\n",
        "covid19_opendata['date'] = pd.to_datetime(covid19_opendata['date'], format=format)\n",
        "covid19_merge1 = covid19_opendata.merge(wearing_mask_in_public_data, \n",
        "                                      left_on = ['country_name','date'],\n",
        "                                      right_on = ['country','DateTime'], how = 'left')\n",
        "\n",
        "\n",
        "remove_cols = ['key', 'country','aggregation_level','locality_code', 'wikidata', 'datacommons', 'country_code', 'subregion1_code', 'subregion1_name', 'subregion2_code', 'subregion2_name', 'locality_name', '3166-1-alpha-2', '3166-1-alpha-3', 'DateTime']\n",
        "\n",
        "covid19_merge1 = covid19_merge1.drop(remove_cols, axis=1)\n",
        "covid19_merge1 = covid19_merge1.fillna(0)\n",
        "\n",
        "prepared_data =  covid19_merge1.copy()\n",
        "\n",
        "## Encode Pays\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoded_countries = LabelEncoder().fit_transform(prepared_data.country_name)\n",
        "prepared_data['country_name'] = encoded_countries\n",
        "\n",
        "## Encode Date\n",
        "dates = prepared_data.date.apply(lambda x: x.strftime('%Y%m%d'))\n",
        "encoded_dates = LabelEncoder().fit_transform(dates)\n",
        "prepared_data['date'] = encoded_dates\n",
        "\n",
        "print(f\"Le dataset contient {len(prepared_data)} enregistrements\")\n",
        "\n",
        "print(\"Sample dataset final:\")\n",
        "print(prepared_data.sample(5))\n",
        "\n",
        "stop = datetime.now()\n",
        "\n",
        "\n",
        "print(\"Temps de chargement et tranformation grand dataset : \", (stop-start).microseconds/1000, \"ms\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le dataset contient 21505 enregistrements\nSample dataset final:\n       date  country_name  new_confirmed  new_deceased  new_recovered  \\\n16562     0           173            1.0           0.0            0.0   \n5803      0            29            4.0           0.0            0.0   \n21113     0           236           -9.0          -1.0            0.0   \n13729     0           141            1.0           0.0            0.0   \n2736      0            29            2.0           0.0            0.0   \n\n       new_tested  total_confirmed  total_deceased  total_recovered  \\\n16562         0.0             22.0             0.0              0.0   \n5803          0.0            184.0             4.0              0.0   \n21113         5.0            473.0             5.0              0.0   \n13729         0.0             62.0             9.0              0.0   \n2736          0.0             70.0             3.0              0.0   \n\n       total_tested  ...  noaa_station  noaa_distance  average_temperature  \\\n16562           0.0  ...  8.468610e+10     104.255060            15.833333   \n5803            0.0  ...  8.684310e+10      83.295298            23.175926   \n21113        5756.0  ...  7.205010e+10      17.461851             2.518519   \n13729           0.0  ...  7.672610e+10      21.499827            14.555556   \n2736            0.0  ...  8.685210e+10      18.392656            26.111111   \n\n       minimum_temperature  maximum_temperature   rainfall  snowfall  \\\n16562            11.122222            19.244444  18.592800       0.0   \n5803             20.222222            28.972222   0.000000       0.0   \n21113             0.728395             6.469136   0.056444       0.0   \n13729             8.037037            21.617284   0.984250       0.0   \n2736             22.411111            30.944444   3.200400       0.0   \n\n       dew_point  relative_humidity  percent_wearing_mask  \n16562  10.486111          65.966233                   0.0  \n5803   20.703704          86.094903                   0.0  \n21113  -1.888889          74.430600                   0.0  \n13729   7.512346          63.829596                   0.0  \n2736   21.066667          74.180949                   0.0  \n\n[5 rows x 96 columns]\nTemps de chargement et tranformation grand dataset :  575.31 ms\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## Entrainement et inférence"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGEFhdbKapMv"
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "# Split Train/Test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = prepared_data.loc[:, prepared_data.columns != 'new_confirmed']\n",
        "y = prepared_data['new_confirmed'].ravel()\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "# Scale des valeurs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Entraintement MLP\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "regr = MLPRegressor(max_iter=10, hidden_layer_sizes=(100, 50, 25, 10, 5), verbose=True).fit(X_train, y_train)\n",
        "\n",
        "# Prédiction et Score\n",
        "score = regr.score(X_test, y_test)\n",
        "\n",
        "stop = datetime.now()\n",
        "\n",
        "print(\"Temps préparation et inférence (ML) : \", (stop-start).microseconds/1000, \"ms\")\n",
        "print(f\"model score: {score}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1913512.05099825\n",
            "Iteration 2, loss = 1912826.56734122\n",
            "Iteration 3, loss = 1908411.19594036\n",
            "Iteration 4, loss = 1882865.65023970\n",
            "Iteration 5, loss = 1813530.45950809\n",
            "Iteration 6, loss = 1718267.55117788\n",
            "Iteration 7, loss = 1633556.30522023\n",
            "Iteration 8, loss = 1598085.43872462\n",
            "Iteration 9, loss = 1549119.57839646\n",
            "Iteration 10, loss = 1493999.51880322\n",
            "Temps préparation et inférence (ML) :  204.974 ms\n",
            "model score: 0.4150852144079944\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## Entrainement et inférence avec Pipeline"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "YdnaYzLzS8oH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1861531.72514722\n",
            "Iteration 2, loss = 1846450.99528227\n",
            "Iteration 3, loss = 1711786.04932882\n",
            "Iteration 4, loss = 1632325.13654811\n",
            "Iteration 5, loss = 1556339.93399228\n",
            "Iteration 6, loss = 1475497.80209052\n",
            "Iteration 7, loss = 1409453.21787756\n",
            "Iteration 8, loss = 1350760.63649599\n",
            "Iteration 9, loss = 1265274.17505883\n",
            "Iteration 10, loss = 1151185.94036052\n",
            "Temps préparation et inférence (ML) :  202.327 ms\n",
            "model score: 0.6564073262602386\n"
          ]
        }
      ],
      "source": [
        "start = datetime.now()\n",
        "\n",
        "# Split Train/Test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = prepared_data.loc[:, prepared_data.columns != 'new_confirmed']\n",
        "y = prepared_data['new_confirmed'].ravel()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Scale des valeurs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Entraintement MLP\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "regr = MLPRegressor(max_iter=10, hidden_layer_sizes=(100, 50, 25, 10, 5), verbose=True)\n",
        "\n",
        "pipeline = Pipeline([('scaler', scaler), ('regressor', regr)])\n",
        "\n",
        "# Exécution du pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Prédiction et Score\n",
        "score = pipeline.score(X_test, y_test)\n",
        "\n",
        "stop = datetime.now()\n",
        "\n",
        "print(\"Temps préparation et inférence (ML) : \", (stop-start).microseconds/1000, \"ms\")\n",
        "print(f\"model score: {score}\")"
      ]
    }
  ]
}